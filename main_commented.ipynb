{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236a477c",
   "metadata": {},
   "source": [
    "# Martiny Family Archive - Knowledge Graph Builder\n",
    "\n",
    "Multimodal analysis pipeline for historical archive processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a0c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment and paths loaded from .env\n",
      "Output directory ensured at: C:\\Users\\wel\\Desktop\\opvn\\outputs\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file (API keys, paths, etc.)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv  \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set protocol buffers implementation (fixes compatibility issues)\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = os.getenv('PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION')\n",
    "\n",
    "# Get Gemini API key - this is required for AI-powered entity extraction\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found. Make sure it's set in your .env file.\")\n",
    "os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\n",
    "\n",
    "# Get paths from environment\n",
    "ARCHIVE_PATH = os.getenv('ARCHIVE_PATH')  \n",
    "OUTPUT_DIR = os.getenv('OUTPUT_DIR')       \n",
    "if not ARCHIVE_PATH or not OUTPUT_DIR:\n",
    "    raise ValueError(\"ARCHIVE_PATH or OUTPUT_DIR not found in .env file.\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Environment and paths loaded from .env\")\n",
    "print(f\"Output directory ensured at: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2166cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized. Output directory: C:\\Users\\wel\\Desktop\\opvn\\outputs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  \n",
    "import cv2  \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from paddleocr import PaddleOCR\n",
    "from docx import Document \n",
    "from PyPDF2 import PdfReader \n",
    "import networkx as nx  \n",
    "from pyvis.network import Network  \n",
    "from rapidfuzz import fuzz\n",
    "import google.generativeai as genai\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Initialized. Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcbf9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data structures ready\n"
     ]
    }
   ],
   "source": [
    "# configuration parameters\n",
    "MAX_SAMPLE_FILES = 20\n",
    "MIN_PHOTO_SIZE = 100  \n",
    "FUZZY_THRESHOLD = 85  \n",
    "\n",
    "#data structures using Python dataclasses\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    \"\"\"Represents a single entity in our knowledge graph (person, place, company, etc.)\"\"\"\n",
    "    id: str  \n",
    "    type: str  \n",
    "    name: str  \n",
    "    properties: Dict \n",
    "    source_file: str \n",
    "    confidence: float = 1.0 \n",
    "\n",
    "@dataclass\n",
    "class Relationship:\n",
    "    \"\"\"Represents a connection between two entities\"\"\"\n",
    "    source_id: str  \n",
    "    target_id: str  \n",
    "    type: str  \n",
    "    properties: Dict  \n",
    "    confidence: float = 1.0 \n",
    "@dataclass\n",
    "class FileMetadata:\n",
    "    \"\"\"Stores information about each processed file\"\"\"\n",
    "    file_id: str\n",
    "    path: str\n",
    "    filename: str\n",
    "    category: str  \n",
    "    file_type: str  \n",
    "    size_bytes: int\n",
    "    ocr_text: Optional[str] = None  \n",
    "    extracted_entities: List[str] = None  \n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.extracted_entities is None:\n",
    "            self.extracted_entities = []\n",
    "\n",
    "#main data stores - these will hold all extracted information\n",
    "entities_db = {}  \n",
    "relationships_db = []  \n",
    "files_db = {}  \n",
    "\n",
    "print(\"Data structures ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ced4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning archive...\n",
      "Found 297 files\n",
      "\n",
      "Category distribution:\n",
      "category\n",
      "album_page       184\n",
      "commercial        44\n",
      "document          23\n",
      "newspaper         12\n",
      "single_photo      10\n",
      "pdf_document      10\n",
      "other              6\n",
      "client_tags        4\n",
      "image              2\n",
      "genealogy          1\n",
      "word_document      1\n",
      "Name: count, dtype: int64\n",
      "  Selected 14 priority files.\n",
      "  Added 6 additional files to reach sample limit.\n",
      "\n",
      "Selected 20 files for processing\n",
      "Catalog saved\n"
     ]
    }
   ],
   "source": [
    "def categorize_file(path: Path) -> str:\n",
    "    \"\"\"Figure out what type of file this is based on its path and name\"\"\"\n",
    "    path_str = str(path).lower()\n",
    "    \n",
    "    # Check path/filename for keywords to determine category\n",
    "    if 'genealog' in path_str:\n",
    "        return 'genealogy'\n",
    "    elif 'album' in path_str and path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "        return 'album_page' \n",
    "    elif 'singole' in path_str or 'fotografie di famiglia' in path_str:\n",
    "        return 'single_photo'\n",
    "    elif 'articoli' in path_str and path.suffix.lower() == '.pdf':\n",
    "        return 'newspaper'\n",
    "    elif 'tag' in path_str and path.suffix.lower() == '.docx':\n",
    "        return 'client_tags'  \n",
    "    elif 'certificati' in path_str or 'lettere' in path_str:\n",
    "        return 'document'\n",
    "    elif 'commerciali' in path_str or 'pubblicitari' in path_str:\n",
    "        return 'commercial'\n",
    "    elif path.suffix.lower() == '.pdf':\n",
    "        return 'pdf_document'\n",
    "    elif path.suffix.lower() == '.docx':\n",
    "        return 'word_document'\n",
    "    elif path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "        return 'image'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def scan_archive(root_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Scan the entire archive folder and catalog all files\"\"\"\n",
    "    root = Path(root_path)\n",
    "    files_data = []\n",
    "    \n",
    "    # Recursively find all files in the archive\n",
    "    for file_path in root.rglob('*'):\n",
    "        # Skip database and text files\n",
    "        if file_path.is_file() and file_path.suffix.lower() not in ['.db', '.txt']:\n",
    "            category = categorize_file(file_path)\n",
    "            files_data.append({\n",
    "                'path': str(file_path),\n",
    "                'filename': file_path.name,\n",
    "                'category': category,\n",
    "                'extension': file_path.suffix.lower(),\n",
    "                'size_kb': file_path.stat().st_size / 1024,\n",
    "                'folder': file_path.parent.name\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(files_data)\n",
    "\n",
    "def select_sample_files(df: pd.DataFrame, max_files: int = 20) -> List[str]:\n",
    "    \"\"\"Select a representative sample of files from different categories\"\"\"\n",
    "    selected = []\n",
    "    \n",
    "    # Priority files: make sure we get at least one of each important type\n",
    "    priority = [\n",
    "        ('genealogy', 2),     # Family trees\n",
    "        ('client_tags', 2),   # Photo metadata\n",
    "        ('album_page', 4),    # Album pages\n",
    "        ('newspaper', 2),     # Historical newspapers\n",
    "        ('document', 2),      # Certificates, letters\n",
    "        ('single_photo', 2),  # Individual photos\n",
    "        ('commercial', 1)     # Business materials\n",
    "    ]\n",
    "    \n",
    "    # Select priority files first\n",
    "    for category, count in priority:\n",
    "        cat_files = df[df['category'] == category]['path'].tolist()\n",
    "        for f in cat_files[:count]:\n",
    "            if f not in selected:\n",
    "                selected.append(f)\n",
    "    \n",
    "    print(f\"  Selected {len(selected)} priority files.\")\n",
    "    \n",
    "    # Fill up to max_files with random additional files\n",
    "    if len(selected) < max_files:\n",
    "        num_needed = max_files - len(selected)\n",
    "        all_files = df['path'].tolist()\n",
    "        remaining_files = [f for f in all_files if f not in selected]\n",
    "        files_to_add = remaining_files[:num_needed]\n",
    "        selected.extend(files_to_add)\n",
    "        print(f\"  Added {len(files_to_add)} additional files to reach sample limit.\")\n",
    "    \n",
    "    return selected[:max_files]\n",
    "\n",
    "# Run the archive scan\n",
    "print(\"Scanning archive...\")\n",
    "archive_df = scan_archive(ARCHIVE_PATH)\n",
    "print(f\"Found {len(archive_df)} files\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(archive_df['category'].value_counts())\n",
    "\n",
    "# select our sample files to process\n",
    "sample_files = select_sample_files(archive_df, MAX_SAMPLE_FILES)\n",
    "print(f\"\\nSelected {len(sample_files)} files for processing\")\n",
    "\n",
    "# save the full catalog for reference\n",
    "archive_df.to_csv(Path(OUTPUT_DIR) / \"file_catalog.csv\", index=False)\n",
    "print(f\"Catalog saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff35e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OCR engine...\n",
      "[2025/11/17 18:13:39] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\wel/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\wel/.paddleocr/whl\\\\rec\\\\latin\\\\latin_PP-OCRv3_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\wel\\\\Desktop\\\\opvn\\\\.venv\\\\Lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\dict\\\\latin_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\wel/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='it', det=True, rec=True, type='ocr', ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "OCR ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize OCR engine for reading text from images\n",
    "print(\"Initializing OCR engine...\")\n",
    "ocr_engine = PaddleOCR(use_angle_cls=True, lang='it') \n",
    "\n",
    "def extract_text_from_image(image_path: str) -> str:\n",
    "    \"\"\"Use OCR to extract text from an image file\"\"\"\n",
    "    try:\n",
    "        result = ocr_engine.ocr(image_path, cls=True)\n",
    "        if result and result[0]:\n",
    "            # Combine all detected text lines\n",
    "            text = ' '.join([line[1][0] for line in result[0]])\n",
    "            return text\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"OCR error for {Path(image_path).name}: {str(e)[:50]}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF (works for PDFs with text layer)\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        # Combine text from all pages\n",
    "        text = ' '.join([page.extract_text() for page in reader.pages])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"PDF error for {Path(pdf_path).name}: {str(e)[:50]}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_docx(docx_path: str) -> str:\n",
    "    \"\"\"Extract text from Word document\"\"\"\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        # Combine all paragraphs\n",
    "        text = ' '.join([para.text for para in doc.paragraphs])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"DOCX error for {Path(docx_path).name}: {str(e)[:50]}\")\n",
    "        return \"\"\n",
    "\n",
    "print(\"OCR ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fb9cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV segmentation ready\n"
     ]
    }
   ],
   "source": [
    "def segment_album_page(image_path: str, output_dir: Path) -> List[str]:\n",
    "    \"\"\"Use computer vision to detect and extract individual photos from album pages\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return []\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding to find photo boundaries\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 21, 10)\n",
    "    \n",
    "    # Morphological operations to clean up the image\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "    \n",
    "    # Find contours (photo boundaries)\n",
    "    contours, _ = cv2.findContours(morph, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter contours by size and aspect ratio\n",
    "    min_area = MIN_PHOTO_SIZE * MIN_PHOTO_SIZE\n",
    "    max_area = img.shape[0] * img.shape[1] * 0.8 \n",
    "    \n",
    "    valid_contours = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if min_area < area < max_area:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            aspect_ratio = w / h if h > 0 else 0\n",
    "            if 0.3 < aspect_ratio < 3.0:\n",
    "                valid_contours.append((x, y, w, h))\n",
    "    \n",
    "    # Save each detected photo segment\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    segmented_paths = []\n",
    "    base_name = Path(image_path).stem\n",
    "    \n",
    "    for idx, (x, y, w, h) in enumerate(valid_contours):\n",
    "        cropped = img[y:y+h, x:x+w]\n",
    "        output_path = output_dir / f\"{base_name}_seg_{idx}.jpg\"\n",
    "        cv2.imwrite(str(output_path), cropped)\n",
    "        segmented_paths.append(str(output_path))\n",
    "    \n",
    "    return segmented_paths\n",
    "\n",
    "print(\"CV segmentation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f8d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini models ready\n"
     ]
    }
   ],
   "source": [
    "# Configure Google Gemini AI\n",
    "api_key = os.environ.get('GEMINI_API_KEY')\n",
    "if not api_key or api_key == 'GEMINI_API_KEY_HERE':\n",
    "    raise ValueError(\"Please set your Gemini API key in the first cell\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Create separate models for vision and text analysis\n",
    "vision_model = genai.GenerativeModel('gemini-2.5-flash-image') \n",
    "text_model = genai.GenerativeModel('gemini-2.5-pro')   \n",
    "\n",
    "def analyze_photo_with_gemini(image_path: str) -> Dict:\n",
    "    \"\"\"Use Gemini Vision AI to extract information from a photo\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Craft a detailed prompt to guide the AI\n",
    "    prompt = \"\"\"Analyze this historical photograph from early 1900s Turin, Italy.\n",
    "Extract and return ONLY valid JSON with this structure:\n",
    "{\n",
    "  \"people\": [{\"name\": \"estimated name or role\", \"description\": \"appearance/context\"}],\n",
    "  \"places\": [{\"name\": \"location name\", \"type\": \"city/building/landmark\"}],\n",
    "  \"objects\": [\"list of significant objects or products visible\"],\n",
    "  \"date_estimate\": \"estimated year or decade\",\n",
    "  \"scene_description\": \"brief description of what's happening\",\n",
    "  \"text_visible\": \"any text visible in the image\"\n",
    "}\n",
    "\n",
    "Output ONLY the JSON object, no other text.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = vision_model.generate_content([prompt, img])\n",
    "        result_text = response.text.strip()\n",
    "        result_text = result_text.replace('```json', '').replace('```', '').strip()\n",
    "        return json.loads(result_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Vision API error: {str(e)[:50]}\")\n",
    "        return {\"people\": [], \"places\": [], \"objects\": [], \"date_estimate\": \"\", \n",
    "                \"scene_description\": \"\", \"text_visible\": \"\"}\n",
    "\n",
    "def analyze_document_with_gemini(text: str, filename: str) -> Dict:\n",
    "    \"\"\"Use Gemini Text AI to extract entities from document text\"\"\"\n",
    "    prompt = f\"\"\"Analyze this historical document text related to the Martiny family of Turin, Italy (early 1900s).\n",
    "\n",
    "Document: {filename}\n",
    "Text: {text[:2000]}\n",
    "\n",
    "Extract entities and return ONLY valid JSON:\n",
    "{{\n",
    "  \"people\": [{{\"name\": \"full name\", \"role\": \"relationship or role\"}}],\n",
    "  \"companies\": [{{\"name\": \"company name\", \"type\": \"industry\"}}],\n",
    "  \"places\": [\"location names\"],\n",
    "  \"dates\": [{{\"date\": \"YYYY or YYYY-MM-DD\", \"event\": \"what happened\"}}],\n",
    "  \"events\": [\"significant events mentioned\"],\n",
    "  \"products\": [\"products or brands mentioned\"]\n",
    "}}\n",
    "\n",
    "Output ONLY the JSON object, no other text.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = text_model.generate_content(prompt)\n",
    "        result_text = response.text.strip()\n",
    "        result_text = result_text.replace('```json', '').replace('```', '').strip()\n",
    "        return json.loads(result_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Text API error: {str(e)[:50]}\")\n",
    "        return {\"people\": [], \"companies\": [], \"places\": [], \"dates\": [], \n",
    "                \"events\": [], \"products\": []}\n",
    "\n",
    "print(\"Gemini models ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71a29fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity normalization ready\n"
     ]
    }
   ],
   "source": [
    "def normalize_name(name: str) -> str:\n",
    "    \"\"\"Clean up entity names (remove uncertainty markers, extra spaces, etc.)\"\"\"\n",
    "    # Remove \"(?)\" uncertainty markers from client metadata\n",
    "    name = re.sub(r'\\s*\\(\\?\\)\\s*', '', name)\n",
    "    # Normalize whitespace\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name\n",
    "\n",
    "def find_similar_entity(name: str, entity_type: str, threshold: int = 85) -> Optional[str]:\n",
    "    \"\"\"Check if we already have a similar entity (prevents duplicates)\"\"\"\n",
    "    normalized = normalize_name(name)\n",
    "    \n",
    "    if entity_type in [\"Photo\", \"Document\"] and (\"_\" in normalized or \".\" in normalized):\n",
    "        for entity_id, entity in entities_db.items():\n",
    "            if entity.type == entity_type and entity.name == normalized:\n",
    "                return entity_id\n",
    "        return None\n",
    "    \n",
    "    for entity_id, entity in entities_db.items():\n",
    "        if entity.type == entity_type:\n",
    "            score = fuzz.ratio(normalized.lower(), entity.name.lower())\n",
    "            if score >= threshold:\n",
    "                return entity_id\n",
    "    return None\n",
    "\n",
    "def add_or_merge_entity(entity_type: str, name: str, properties: Dict, \n",
    "                        source_file: str, confidence: float = 1.0) -> str:\n",
    "    \"\"\"Add a new entity or merge with existing one if it's a duplicate\"\"\"\n",
    "    existing_id = find_similar_entity(name, entity_type)\n",
    "    \n",
    "    if existing_id:\n",
    "        # Entity already exists - merge properties\n",
    "        existing = entities_db[existing_id]\n",
    "        for k, v in properties.items():\n",
    "            if k not in existing.properties:\n",
    "                existing.properties[k] = v\n",
    "        return existing_id\n",
    "    else:\n",
    "        # Create new entity\n",
    "        entity_id = f\"{entity_type}_{len([e for e in entities_db.values() if e.type == entity_type]) + 1}\"\n",
    "        entity = Entity(\n",
    "            id=entity_id,\n",
    "            type=entity_type,\n",
    "            name=normalize_name(name),\n",
    "            properties=properties,\n",
    "            source_file=source_file,\n",
    "            confidence=confidence\n",
    "        )\n",
    "        entities_db[entity_id] = entity\n",
    "        return entity_id\n",
    "\n",
    "print(\"Entity normalization ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1aa3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeline ready\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def relationship_exists(source_id: str, target_id: str, rel_type: str) -> bool:\n",
    "    \"\"\"Check if a relationship already exists (avoid creating duplicates)\"\"\"\n",
    "    for rel in relationships_db:\n",
    "        if (rel.source_id == source_id and \n",
    "            rel.target_id == target_id and \n",
    "            rel.type == rel_type):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def add_relationship(source_id: str, target_id: str, rel_type: str, properties: Dict):\n",
    "    \"\"\"Create a new relationship between two entities\"\"\"\n",
    "    if not relationship_exists(source_id, target_id, rel_type):\n",
    "        relationships_db.append(Relationship(\n",
    "            source_id=source_id,\n",
    "            target_id=target_id,\n",
    "            type=rel_type,\n",
    "            properties=properties\n",
    "        ))\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def process_file(file_path: str, category: str) -> None:\n",
    "    \"\"\"Main processing function - extracts entities and relationships from a single file\"\"\"\n",
    "    path = Path(file_path)\n",
    "    file_id = f\"FILE_{len(files_db) + 1}\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {path.name} ({category})\")\n",
    "    \n",
    "    # metadata record for this file\n",
    "    metadata = FileMetadata(\n",
    "        file_id=file_id,\n",
    "        path=str(path),\n",
    "        filename=path.name,\n",
    "        category=category,\n",
    "        file_type=path.suffix.lower(),\n",
    "        size_bytes=path.stat().st_size\n",
    "    )\n",
    "    \n",
    "    # entity for the file itself (Document or Photo)\n",
    "    file_entity_id = add_or_merge_entity(\n",
    "        entity_type=\"Document\" if category != \"album_page\" else \"Photo\",\n",
    "        name=path.stem,\n",
    "        properties={\"category\": category, \"file_type\": path.suffix},\n",
    "        source_file=file_id\n",
    "    )\n",
    "    \n",
    "    # Process based on file category\n",
    "    if category == \"album_page\":\n",
    "        seg_dir = Path(OUTPUT_DIR) / \"segmented\" / path.stem\n",
    "        segmented_photos = segment_album_page(str(path), seg_dir)\n",
    "        print(f\"  Segmented into {len(segmented_photos)} photos\")\n",
    "        \n",
    "        for idx, seg_path in enumerate(segmented_photos[:3], 1):\n",
    "            print(f\"  Analyzing segment {idx}...\")\n",
    "            photo_data = analyze_photo_with_gemini(seg_path)\n",
    "            print(f\"    Gemini response: {json.dumps(photo_data, indent=2)}\")\n",
    "            \n",
    "            # Extract people from the photo\n",
    "            for person in photo_data.get('people', []):\n",
    "                person_id = add_or_merge_entity(\n",
    "                    \"Person\", person.get('name', 'Unknown'),\n",
    "                    {\"description\": person.get('description', '')},\n",
    "                    file_id\n",
    "                )\n",
    "                # \"appears_in\" relationship\n",
    "                if add_relationship(person_id, file_entity_id, \"appears_in\", \n",
    "                                  {\"context\": photo_data.get('scene_description', '')}):\n",
    "                    print(f\"      ✓ {person.get('name')} appears_in {path.stem}\")\n",
    "            \n",
    "            # Extract places from the photo\n",
    "            for place in photo_data.get('places', []):\n",
    "                place_id = add_or_merge_entity(\n",
    "                    \"Place\", place.get('name', ''),\n",
    "                    {\"type\": place.get('type', '')},\n",
    "                    file_id\n",
    "                )\n",
    "                #  \"located_at\" relationship\n",
    "                if add_relationship(file_entity_id, place_id, \"located_at\", {}):\n",
    "                    print(f\" {path.stem} located_at {place.get('name')}\")\n",
    "    \n",
    "    elif category in [\"newspaper\", \"pdf_document\", \"genealogy\"]:\n",
    "        # Extract text from PDF\n",
    "        text = extract_text_from_pdf(str(path))\n",
    "        print(f\"  PDF text extracted: {len(text)} chars\")\n",
    "        \n",
    "        # If PDF has very little text, it's probably scanned - use OCR\n",
    "        if len(text) < 100:\n",
    "            print(f\"  PDF has little text, trying OCR...\")\n",
    "            ocr_text = extract_text_from_image(str(path))\n",
    "            if ocr_text:\n",
    "                text = ocr_text\n",
    "                print(f\"  OCR extracted: {len(text)} chars\")\n",
    "        \n",
    "        metadata.ocr_text = text[:500]  \n",
    "        \n",
    "        # Only process if we got meaningful text\n",
    "        if len(text) > 50:\n",
    "            print(f\"  Calling Gemini with {len(text)} chars...\")\n",
    "            doc_data = analyze_document_with_gemini(text, path.name)\n",
    "            print(f\"  Gemini response: {json.dumps(doc_data, indent=2)}\")\n",
    "            \n",
    "            # Extract people mentioned in the document\n",
    "            for person in doc_data.get('people', []):\n",
    "                person_id = add_or_merge_entity(\n",
    "                    \"Person\", person.get('name', ''),\n",
    "                    {\"role\": person.get('role', '')},\n",
    "                    file_id\n",
    "                )\n",
    "                if add_relationship(file_entity_id, person_id, \"mentions\", {}):\n",
    "                    print(f\" - mentions {person.get('name')}\")\n",
    "            \n",
    "            # Extract companies mentioned\n",
    "            for company in doc_data.get('companies', []):\n",
    "                company_id = add_or_merge_entity(\n",
    "                    \"Company\", company.get('name', ''),\n",
    "                    {\"type\": company.get('type', '')},\n",
    "                    file_id\n",
    "                )\n",
    "                if add_relationship(file_entity_id, company_id, \"mentions\", {}):\n",
    "                    print(f\"      - mentions {company.get('name')}\")\n",
    "        else:\n",
    "            print(f\" -- Skipping - not enough text ({len(text)} chars)\")\n",
    "    \n",
    "    elif category == \"client_tags\":\n",
    "        # Process Word documents with photo metadata\n",
    "        text = extract_text_from_docx(str(path))\n",
    "        print(f\"  DOCX text extracted: {len(text)} chars\")\n",
    "        metadata.ocr_text = text[:500]\n",
    "        \n",
    "        if len(text) > 50:\n",
    "            print(f\"  Calling Gemini...\")\n",
    "            doc_data = analyze_document_with_gemini(text, path.name)\n",
    "            print(f\"  Gemini response: {json.dumps(doc_data, indent=2)}\")\n",
    "            \n",
    "            for person in doc_data.get('people', []):\n",
    "                person_id = add_or_merge_entity(\n",
    "                    \"Person\", person.get('name', ''),\n",
    "                    {\"role\": person.get('role', '')},\n",
    "                    file_id\n",
    "                )\n",
    "                add_relationship(file_entity_id, person_id, \"mentions\", {})\n",
    "    \n",
    "    elif category == \"single_photo\":\n",
    "        # Process individual photos\n",
    "        print(f\"  Analyzing photo...\")\n",
    "        photo_data = analyze_photo_with_gemini(str(path))\n",
    "        print(f\"  Gemini response: {json.dumps(photo_data, indent=2)}\")\n",
    "        \n",
    "        for person in photo_data.get('people', []):\n",
    "            person_id = add_or_merge_entity(\n",
    "                \"Person\", person.get('name', 'Unknown'),\n",
    "                {\"description\": person.get('description', '')},\n",
    "                file_id\n",
    "            )\n",
    "            if add_relationship(person_id, file_entity_id, \"appears_in\", {}):\n",
    "                print(f\"       {person.get('name')} appears_in {path.stem}\")\n",
    "    \n",
    "    elif category == \"document\":\n",
    "        # For image-based documents (scanned certificates), use OCR\n",
    "        print(f\"  Running OCR on document image...\")\n",
    "        text = extract_text_from_image(str(path))\n",
    "        print(f\"  OCR extracted: {len(text)} chars\")\n",
    "        metadata.ocr_text = text[:500]\n",
    "        \n",
    "        if len(text) > 50:\n",
    "            doc_data = analyze_document_with_gemini(text, path.name)\n",
    "            print(f\"  Gemini response: {json.dumps(doc_data, indent=2)}\")\n",
    "    \n",
    "    # Save file metadata\n",
    "    files_db[file_id] = metadata\n",
    "    \n",
    "    # Small delay to avoid hitting API rate limits\n",
    "    time.sleep(2)\n",
    "    print(f\"  Completed\")\n",
    "\n",
    "print(\"Processing pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f8b32da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING PIPELINE EXECUTION\n",
      "============================================================\n",
      "\n",
      "[1/20] \n",
      "============================================================\n",
      "Processing: 2025 - 10 - 17 alberi geneologici famiglia Martiny.pdf (genealogy)\n",
      "  PDF text extracted: 2 chars\n",
      "  PDF has little text, trying OCR...\n",
      "[2025/11/17 18:13:54] ppocr DEBUG: dt_boxes num : 43, elapsed : 4.9650022983551025\n",
      "[2025/11/17 18:13:56] ppocr DEBUG: cls num  : 43, elapsed : 1.539111852645874\n",
      "[2025/11/17 18:14:09] ppocr DEBUG: rec_res num  : 43, elapsed : 13.203935146331787\n",
      "[2025/11/17 18:14:11] ppocr DEBUG: dt_boxes num : 17, elapsed : 1.7567613124847412\n",
      "[2025/11/17 18:14:12] ppocr DEBUG: cls num  : 17, elapsed : 0.9484336376190186\n",
      "[2025/11/17 18:14:13] ppocr DEBUG: rec_res num  : 17, elapsed : 1.192706823348999\n",
      "[2025/11/17 18:14:14] ppocr DEBUG: dt_boxes num : 9, elapsed : 0.4354534149169922\n",
      "[2025/11/17 18:14:14] ppocr DEBUG: cls num  : 9, elapsed : 0.24342989921569824\n",
      "[2025/11/17 18:14:15] ppocr DEBUG: rec_res num  : 9, elapsed : 0.7516500949859619\n",
      "  OCR extracted: 290 chars\n",
      "  Calling Gemini with 290 chars...\n",
      "  Gemini response: {\n",
      "  \"people\": [\n",
      "    {\n",
      "      \"name\": \"Giovanni Martiny\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Johann\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Johanne\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Alwine\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Ludovico\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Heinrich\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Elisabeth Martiny\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Ernst Willeke\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Masche\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Roberto Collasius\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Erna Willeke\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Francesco Martiny\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Clara Collasius\",\n",
      "      \"role\": \"family member\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Giovanni Martiny\",\n",
      "      \"role\": \"child\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Walter Martiny\",\n",
      "      \"role\": \"child\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Ernesto Martiny\",\n",
      "      \"role\": \"child\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Frida Martiny\",\n",
      "      \"role\": \"child\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Luigi Martiny\",\n",
      "      \"role\": \"child\"\n",
      "    }\n",
      "  ],\n",
      "  \"companies\": [],\n",
      "  \"places\": [],\n",
      "  \"dates\": [\n",
      "    {\n",
      "      \"date\": \"1830-1892\",\n",
      "      \"event\": \"Lifespan of Giovanni Martiny\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"1861-1899\",\n",
      "      \"event\": \"Lifespan of an unidentified individual\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"1905\",\n",
      "      \"event\": \"Death of an unidentified individual\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"1883\",\n",
      "      \"event\": \"Birth year of Giovanni Martiny\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"1885\",\n",
      "      \"event\": \"Birth year of Walter Martiny\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"1887\",\n",
      "      \"event\": \"Birth year of Ernesto Martiny\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"1890\",\n",
      "      \"event\": \"Birth year of Frida Martiny\"\n",
      "    }\n",
      "  ],\n",
      "  \"events\": [\n",
      "    \"Genealogical record of births and deaths for the Martiny, Willeke, and Collasius families.\"\n",
      "  ],\n",
      "  \"products\": []\n",
      "}\n",
      " - mentions Giovanni Martiny\n",
      " - mentions Johann\n",
      " - mentions Alwine\n",
      " - mentions Ludovico\n",
      " - mentions Heinrich\n",
      " - mentions Elisabeth Martiny\n",
      " - mentions Ernst Willeke\n",
      " - mentions Masche\n",
      " - mentions Roberto Collasius\n",
      " - mentions Francesco Martiny\n",
      " - mentions Clara Collasius\n",
      " - mentions Walter Martiny\n",
      " - mentions Ernesto Martiny\n",
      " - mentions Frida Martiny\n",
      " - mentions Luigi Martiny\n",
      "  Completed\n",
      "\n",
      "[2/20] \n",
      "============================================================\n",
      "Processing: 2025 - 10 - 20 Tag album giovani musicisti Martiny(1).docx (client_tags)\n",
      "  DOCX text extracted: 2200 chars\n",
      "  Calling Gemini...\n",
      "  Gemini response: {\n",
      "  \"people\": [\n",
      "    {\n",
      "      \"name\": \"Francesco Martiny\",\n",
      "      \"role\": \"Subject of photographs\"\n",
      "    }\n",
      "  ],\n",
      "  \"companies\": [],\n",
      "  \"places\": [\n",
      "    \"Perugia\"\n",
      "  ],\n",
      "  \"dates\": [\n",
      "    {\n",
      "      \"date\": \"1929-09\",\n",
      "      \"event\": \"Photographs taken for the 'Piccoli musicisti' album\"\n",
      "    }\n",
      "  ],\n",
      "  \"events\": [\n",
      "    \"A series of photographs were taken for an album titled 'Piccoli musicisti' (Young Musicians), featuring Francesco Martiny and other unknown individuals.\"\n",
      "  ],\n",
      "  \"products\": [\n",
      "    \"Album: Piccoli musicisti\"\n",
      "  ]\n",
      "}\n",
      "  Completed\n",
      "\n",
      "[3/20] \n",
      "============================================================\n",
      "Processing: 2025 - 10 - 20 Tag album giovani musicisti Martiny.docx (client_tags)\n",
      "  DOCX text extracted: 2005 chars\n",
      "  Calling Gemini...\n",
      "  Gemini response: {\n",
      "  \"people\": [\n",
      "    {\n",
      "      \"name\": \"Francesco Martiny\",\n",
      "      \"role\": \"Subject in photographs / Possible author\"\n",
      "    }\n",
      "  ],\n",
      "  \"companies\": [],\n",
      "  \"places\": [\n",
      "    \"Perugia\"\n",
      "  ],\n",
      "  \"dates\": [\n",
      "    {\n",
      "      \"date\": \"1929-09\",\n",
      "      \"event\": \"Date of photographs in the 'Piccoli musicisti' album\"\n",
      "    }\n",
      "  ],\n",
      "  \"events\": [\n",
      "    \"Photo session for the 'Piccoli musicisti' (Little musicians) album\"\n",
      "  ],\n",
      "  \"products\": [\n",
      "    \"Album: Piccoli musicisti\"\n",
      "  ]\n",
      "}\n",
      "  Completed\n",
      "\n",
      "[4/20] \n",
      "============================================================\n",
      "Processing: Album A_000.JPG (album_page)\n",
      "  Segmented into 1 photos\n",
      "  Analyzing segment 1...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [],\n",
      "  \"objects\": [\n",
      "    \"leather-bound book\",\n",
      "    \"album\",\n",
      "    \"cover\"\n",
      "  ],\n",
      "  \"date_estimate\": \"early 1900s\",\n",
      "  \"scene_description\": \"A close-up, top-down view of a rectangular, dark red, leather-bound book or album. The cover features an embossed, ornate border and large, intertwined initials 'M H' in the center. The leather shows signs of age and wear, particularly along the spine on the left.\",\n",
      "  \"text_visible\": \"M H\"\n",
      "}\n",
      "  Completed\n",
      "\n",
      "[5/20] \n",
      "============================================================\n",
      "Processing: Album A_001.JPG (album_page)\n",
      "  Segmented into 0 photos\n",
      "  Completed\n",
      "\n",
      "[6/20] \n",
      "============================================================\n",
      "Processing: Album A_006.JPG (album_page)\n",
      "  Segmented into 4 photos\n",
      "  Analyzing segment 1...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [],\n",
      "  \"objects\": [\n",
      "    \"wooden planks\",\n",
      "    \"timber\"\n",
      "  ],\n",
      "  \"date_estimate\": \"unknown\",\n",
      "  \"scene_description\": \"A close-up, black and white image showing stacked wooden planks or timber, possibly part of a construction or material storage. The planks are rough-hewn and have visible grain and texture. There are no people or specific identifiable landmarks.\",\n",
      "  \"text_visible\": \"\"\n",
      "}\n",
      "  Analyzing segment 2...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [\n",
      "    {\n",
      "      \"name\": \"Turin\",\n",
      "      \"type\": \"city\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Walter Martiny Industria Gomma\",\n",
      "      \"type\": \"building\"\n",
      "    }\n",
      "  ],\n",
      "  \"objects\": [\n",
      "    \"construction site\",\n",
      "    \"wooden scaffolding\",\n",
      "    \"concrete pillars\",\n",
      "    \"shovels\",\n",
      "    \"wooden planks\",\n",
      "    \"building materials\"\n",
      "  ],\n",
      "  \"date_estimate\": \"1900s-1920s\",\n",
      "  \"scene_description\": \"A construction site in Turin, Italy, showing the foundations and early stages of a building project with numerous concrete pillars and wooden scaffolding. In the background, there is an existing building with the name 'Walter Martiny Industria Gomma' visible on its facade.\",\n",
      "  \"text_visible\": \"SOCIETA' ANONIMA WALTER MARTINY INDUSTRIA GOMMA\"\n",
      "}\n",
      " Album A_006 located_at Turin\n",
      " Album A_006 located_at Walter Martiny Industria Gomma\n",
      "  Analyzing segment 3...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [\n",
      "    {\n",
      "      \"name\": \"Turin\",\n",
      "      \"type\": \"city\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Societ\\u00e0 Anonima Walter Martiny Industria Gomma\",\n",
      "      \"type\": \"building\"\n",
      "    }\n",
      "  ],\n",
      "  \"objects\": [\n",
      "    \"construction materials\",\n",
      "    \"wooden planks\",\n",
      "    \"concrete pillars\",\n",
      "    \"building foundations\",\n",
      "    \"utility lines\",\n",
      "    \"factory buildings\",\n",
      "    \"chimney\"\n",
      "  ],\n",
      "  \"date_estimate\": \"1910s-1920s\",\n",
      "  \"scene_description\": \"A construction site in front of a factory building, with numerous concrete pillars erected for a new structure. Lumber and construction debris are scattered on the ground. In the background, factory buildings with multiple windows and a prominent chimney are visible under a clear sky.\",\n",
      "  \"text_visible\": \"SOCIET\\u00c0 ANONIMA WALTER MARTINY INDUSTRIA GOMMA\"\n",
      "}\n",
      " Album A_006 located_at Società Anonima Walter Martiny Industria Gomma\n",
      "  Completed\n",
      "\n",
      "[7/20] \n",
      "============================================================\n",
      "Processing: Album A_014.JPG (album_page)\n",
      "  Segmented into 0 photos\n",
      "  Completed\n",
      "\n",
      "[8/20] \n",
      "============================================================\n",
      "Processing: 2025 - 10 - 17 articolo di giornalista conoscente di Roberto sulla famiglia Martiny.pdf (newspaper)\n",
      "  PDF text extracted: 5 chars\n",
      "  PDF has little text, trying OCR...\n",
      "[2025/11/17 18:16:06] ppocr DEBUG: dt_boxes num : 45, elapsed : 1.6741199493408203\n",
      "[2025/11/17 18:16:08] ppocr DEBUG: cls num  : 45, elapsed : 1.9821453094482422\n",
      "[2025/11/17 18:16:28] ppocr DEBUG: rec_res num  : 45, elapsed : 19.330423593521118\n",
      "[2025/11/17 18:16:29] ppocr DEBUG: dt_boxes num : 47, elapsed : 1.1450846195220947\n",
      "[2025/11/17 18:16:29] ppocr DEBUG: cls num  : 47, elapsed : 0.4550313949584961\n",
      "[2025/11/17 18:16:43] ppocr DEBUG: rec_res num  : 47, elapsed : 13.445024251937866\n",
      "[2025/11/17 18:16:43] ppocr DEBUG: dt_boxes num : 46, elapsed : 0.5100374221801758\n",
      "[2025/11/17 18:16:44] ppocr DEBUG: cls num  : 46, elapsed : 0.5425450801849365\n",
      "[2025/11/17 18:16:58] ppocr DEBUG: rec_res num  : 46, elapsed : 14.165044784545898\n",
      "  OCR extracted: 3579 chars\n",
      "  Calling Gemini with 3579 chars...\n",
      "  Gemini response: {\n",
      "  \"people\": [\n",
      "    {\n",
      "      \"name\": \"Roberto\",\n",
      "      \"role\": \"acquaintance of the journalist\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Francesco Martiny\",\n",
      "      \"role\": \"Entrepreneur and citizen of Turin\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Edoardo Bender\",\n",
      "      \"role\": \"Business partner of Francesco Martiny\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Charles Goodyear\",\n",
      "      \"role\": \"Inventor\"\n",
      "    }\n",
      "  ],\n",
      "  \"companies\": [\n",
      "    {\n",
      "      \"name\": \"Bender e Martiny\",\n",
      "      \"type\": \"Asbestos and rubber products\"\n",
      "    }\n",
      "  ],\n",
      "  \"places\": [\n",
      "    \"Turin\",\n",
      "    \"Nole\",\n",
      "    \"Piedmont\",\n",
      "    \"Nole Canavese\",\n",
      "    \"Hildelsheim\",\n",
      "    \"Lower Saxony\",\n",
      "    \"Italy\"\n",
      "  ],\n",
      "  \"dates\": [\n",
      "    {\n",
      "      \"date\": \"2025-10-17\",\n",
      "      \"event\": \"Date of the journalistic article\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"1840\",\n",
      "      \"event\": \"Charles Goodyear patents the rubber vulcanization process\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"1853\",\n",
      "      \"event\": \"Birth of Francesco Martiny\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"1875\",\n",
      "      \"event\": \"The 'Bender e Martiny' factory is documented in Nole Canavese\"\n",
      "    }\n",
      "  ],\n",
      "  \"events\": [\n",
      "    \"A tragic road accident in the early 1900s\",\n",
      "    \"Italian Risorgimento\",\n",
      "    \"Industrialization of Piedmont in the second half of the 1800s\",\n",
      "    \"Unification of Italy\"\n",
      "  ],\n",
      "  \"products\": [\n",
      "    \"amianto\",\n",
      "    \"Superga\",\n",
      "    \"calzatura sportiva\",\n",
      "    \"articoli in amianto e caucciu\",\n",
      "    \"caucciu\",\n",
      "    \"gomma\"\n",
      "  ]\n",
      "}\n",
      " - mentions Roberto\n",
      " - mentions Francesco Martiny\n",
      " - mentions Edoardo Bender\n",
      " - mentions Charles Goodyear\n",
      "      - mentions Bender e Martiny\n",
      "  Completed\n",
      "\n",
      "[9/20] \n",
      "============================================================\n",
      "Processing: 1154_01_1929_0059_0005.pdf (newspaper)\n",
      "  PDF text extracted: 34 chars\n",
      "  PDF has little text, trying OCR...\n",
      "[2025/11/17 18:17:20] ppocr DEBUG: dt_boxes num : 26, elapsed : 2.952216863632202\n",
      "[2025/11/17 18:17:26] ppocr DEBUG: cls num  : 26, elapsed : 5.21438193321228\n",
      "[2025/11/17 18:17:31] ppocr DEBUG: rec_res num  : 26, elapsed : 5.312391757965088\n",
      "  OCR extracted: 445 chars\n",
      "  Calling Gemini with 445 chars...\n",
      "  Gemini response: {\n",
      "  \"people\": [\n",
      "    {\n",
      "      \"name\": \"Italo Balbo\",\n",
      "      \"role\": \"S.E. (Sua Eccellenza)\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Ministro dell'Aviazione francese\",\n",
      "      \"role\": \"French Minister of Aviation\"\n",
      "    }\n",
      "  ],\n",
      "  \"companies\": [\n",
      "    {\n",
      "      \"name\": \"LA STAMPA\",\n",
      "      \"type\": \"Newspaper\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"PHILIPS\",\n",
      "      \"type\": \"Electronics\"\n",
      "    }\n",
      "  ],\n",
      "  \"places\": [\n",
      "    \"Torino\",\n",
      "    \"Oulx\",\n",
      "    \"TEATRO ALFIERI\"\n",
      "  ],\n",
      "  \"dates\": [\n",
      "    {\n",
      "      \"date\": \"1929\",\n",
      "      \"event\": \"Publication of the newspaper article, based on document name\"\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"1867\",\n",
      "      \"event\": \"Founding year of the La Stampa Historical Archive\"\n",
      "    }\n",
      "  ],\n",
      "  \"events\": [\n",
      "    \"Italo Balbo's arrival in Turin\",\n",
      "    \"Meeting between Italo Balbo and the French Minister of Aviation\",\n",
      "    \"New initiatives for demographic development\",\n",
      "    \"Mezza Quaresima (Mid-Lent)\"\n",
      "  ],\n",
      "  \"products\": [\n",
      "    \"ANTRACITE\",\n",
      "    \"Valvole PHILIPS\"\n",
      "  ]\n",
      "}\n",
      " - mentions Italo Balbo\n",
      " - mentions Ministro dell'Aviazione francese\n",
      "      - mentions LA STAMPA\n",
      "      - mentions PHILIPS\n",
      "  Completed\n",
      "\n",
      "[10/20] \n",
      "============================================================\n",
      "Processing: Handelshochschule_diploma.JPG (document)\n",
      "  Running OCR on document image...\n",
      "[2025/11/17 18:17:58] ppocr DEBUG: dt_boxes num : 14, elapsed : 2.125155448913574\n",
      "[2025/11/17 18:17:59] ppocr DEBUG: cls num  : 14, elapsed : 0.20701313018798828\n",
      "[2025/11/17 18:18:01] ppocr DEBUG: rec_res num  : 14, elapsed : 2.3451735973358154\n",
      "  OCR extracted: 139 chars\n",
      "  Gemini response: {\n",
      "  \"people\": [\n",
      "    {\n",
      "      \"name\": \"Martiny\",\n",
      "      \"role\": \"Studierender der Handelswissenschaften\"\n",
      "    }\n",
      "  ],\n",
      "  \"companies\": [\n",
      "    {\n",
      "      \"name\": \"Handelshochschule zu Leipzig\",\n",
      "      \"type\": \"Education\"\n",
      "    }\n",
      "  ],\n",
      "  \"places\": [\n",
      "    \"Leipzig\"\n",
      "  ],\n",
      "  \"dates\": [],\n",
      "  \"events\": [],\n",
      "  \"products\": []\n",
      "}\n",
      "  Completed\n",
      "\n",
      "[11/20] \n",
      "============================================================\n",
      "Processing: Handelshochschule_pag1-4.JPG (document)\n",
      "  Running OCR on document image...\n",
      "[2025/11/17 18:18:23] ppocr DEBUG: dt_boxes num : 19, elapsed : 2.1471588611602783\n",
      "[2025/11/17 18:18:28] ppocr DEBUG: cls num  : 19, elapsed : 4.275314092636108\n",
      "[2025/11/17 18:18:32] ppocr DEBUG: rec_res num  : 19, elapsed : 4.250312805175781\n",
      "  OCR extracted: 548 chars\n",
      "  Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"companies\": [\n",
      "    {\n",
      "      \"name\": \"Handelshochschule zu Leipzig\",\n",
      "      \"type\": \"Education\"\n",
      "    }\n",
      "  ],\n",
      "  \"places\": [\n",
      "    \"Leipzig\"\n",
      "  ],\n",
      "  \"dates\": [\n",
      "    {\n",
      "      \"date\": \"1905\",\n",
      "      \"event\": \"Issuance of a leaving certificate (Abgangszeugnis), based on the text '29 Cuk 190s'\"\n",
      "    }\n",
      "  ],\n",
      "  \"events\": [\n",
      "    \"Matriculation at the Handelshochschule zu Leipzig\",\n",
      "    \"Attendance of lectures and commercial exercises\",\n",
      "    \"Issuance of a leaving certificate (Abgangszeugnis)\"\n",
      "  ],\n",
      "  \"products\": []\n",
      "}\n",
      "  Completed\n",
      "\n",
      "[12/20] \n",
      "============================================================\n",
      "Processing: 2025-10-20 TAG FOTO SINGOLE.docx (single_photo)\n",
      "  Analyzing photo...\n",
      "  Error: cannot identify image file 'C:\\\\Users\\\\wel\\\\Desktop\\\\OPV\\\\wetransfer_famiglia-martiny_2025-11-14_163\n",
      "\n",
      "[13/20] \n",
      "============================================================\n",
      "Processing: IMG_8959.JPG (single_photo)\n",
      "  Analyzing photo...\n",
      "  Gemini response: {\n",
      "  \"people\": [\n",
      "    {\n",
      "      \"name\": \"Woman\",\n",
      "      \"description\": \"Appears to be a young woman with dark hair, styled in a bob, wearing a light-colored, possibly satin or silk, dress with short sleeves and some detailing on the bodice. She is holding a small bouquet of roses and is seated sideways, looking towards the camera.\"\n",
      "    }\n",
      "  ],\n",
      "  \"places\": [],\n",
      "  \"objects\": [\n",
      "    \"dress\",\n",
      "    \"roses\",\n",
      "    \"chair\"\n",
      "  ],\n",
      "  \"date_estimate\": \"1900s-1920s\",\n",
      "  \"scene_description\": \"A portrait of a young woman, likely taken in a studio, posing elegantly with roses.\",\n",
      "  \"text_visible\": \"\"\n",
      "}\n",
      "       Woman appears_in IMG_8959\n",
      "  Completed\n",
      "\n",
      "[14/20] \n",
      "============================================================\n",
      "Processing: Ami Guarn_000.JPG (commercial)\n",
      "  Completed\n",
      "\n",
      "[15/20] \n",
      "============================================================\n",
      "Processing: Album A_017.JPG (album_page)\n",
      "  Segmented into 7 photos\n",
      "  Analyzing segment 1...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [],\n",
      "  \"objects\": [],\n",
      "  \"date_estimate\": \"early 1900s\",\n",
      "  \"scene_description\": \"The provided image is too dark and pixelated to discern any details about people, places, objects, or text. It appears to be two light-colored rectangular shapes against a dark background, possibly damaged or heavily compressed.\",\n",
      "  \"text_visible\": \"\"\n",
      "}\n",
      "  Analyzing segment 2...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [\n",
      "    {\n",
      "      \"name\": \"Turin\",\n",
      "      \"type\": \"city\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Workshop or factory interior\",\n",
      "      \"type\": \"building\"\n",
      "    }\n",
      "  ],\n",
      "  \"objects\": [\n",
      "    \"steering wheel\",\n",
      "    \"car chassis parts\",\n",
      "    \"wooden planks\",\n",
      "    \"machinery\"\n",
      "  ],\n",
      "  \"date_estimate\": \"1900s-1910s\",\n",
      "  \"scene_description\": \"A close-up view of what appears to be an early automobile's steering wheel and dashboard, with other vehicle components and wooden planks visible in a workshop or factory setting.\",\n",
      "  \"text_visible\": \"None\"\n",
      "}\n",
      " Album A_017 located_at Turin\n",
      " Album A_017 located_at Workshop or factory interior\n",
      "  Analyzing segment 3...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [\n",
      "    {\n",
      "      \"name\": \"Turin\",\n",
      "      \"type\": \"city\"\n",
      "    }\n",
      "  ],\n",
      "  \"objects\": [\n",
      "    \"wooden frame/ladder-like structure\",\n",
      "    \"large flat tables or surfaces\",\n",
      "    \"piles of fabric or cloth\",\n",
      "    \"windows\",\n",
      "    \"various indistinguishable items in the background\"\n",
      "  ],\n",
      "  \"date_estimate\": \"1900s-1920s\",\n",
      "  \"scene_description\": \"An interior shot of what appears to be a workshop or factory, possibly a textile or printing operation, with large tables, a wooden structure, and piles of fabric. The walls show signs of wear and age.\",\n",
      "  \"text_visible\": \"B\"\n",
      "}\n",
      "  Completed\n",
      "\n",
      "[16/20] \n",
      "============================================================\n",
      "Processing: Album A_021.JPG (album_page)\n",
      "  Segmented into 0 photos\n",
      "  Completed\n",
      "\n",
      "[17/20] \n",
      "============================================================\n",
      "Processing: Album A_023.JPG (album_page)\n",
      "  Segmented into 0 photos\n",
      "  Completed\n",
      "\n",
      "[18/20] \n",
      "============================================================\n",
      "Processing: Album A_024.JPG (album_page)\n",
      "  Segmented into 9 photos\n",
      "  Analyzing segment 1...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [],\n",
      "  \"objects\": [\n",
      "    \"accordion bellows\",\n",
      "    \"fabric casing\"\n",
      "  ],\n",
      "  \"date_estimate\": \"unknown\",\n",
      "  \"scene_description\": \"A close-up of the edge of an object, possibly an accordion or a book with a corrugated spine, showing fabric and rigid material.\",\n",
      "  \"text_visible\": \"\"\n",
      "}\n",
      "  Analyzing segment 2...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [],\n",
      "  \"objects\": [\n",
      "    \"machinery\",\n",
      "    \"chains\",\n",
      "    \"gears\",\n",
      "    \"fabric or covering\"\n",
      "  ],\n",
      "  \"date_estimate\": \"early 1900s\",\n",
      "  \"scene_description\": \"A close-up, monochromatic view of industrial machinery, likely showing gears, chains, and possibly part of a textile or manufacturing process. The image focuses on mechanical components.\",\n",
      "  \"text_visible\": \"\"\n",
      "}\n",
      "  Analyzing segment 3...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [],\n",
      "  \"objects\": [\n",
      "    \"machine parts\",\n",
      "    \"wheels\",\n",
      "    \"fabric/drapery\",\n",
      "    \"bell\"\n",
      "  ],\n",
      "  \"date_estimate\": \"1900s-1920s\",\n",
      "  \"scene_description\": \"A close-up, black and white industrial scene, possibly inside a factory or workshop, showing various machine components and what appears to be textile material.\",\n",
      "  \"text_visible\": \"\"\n",
      "}\n",
      "  Completed\n",
      "\n",
      "[19/20] \n",
      "============================================================\n",
      "Processing: Album A_038.JPG (album_page)\n",
      "  Segmented into 11 photos\n",
      "  Analyzing segment 1...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [],\n",
      "  \"objects\": [],\n",
      "  \"date_estimate\": \"unknown\",\n",
      "  \"scene_description\": \"The image appears to be a grey, textured surface, possibly a close-up of something abstract or out of focus. It is impossible to determine any historical context, people, objects, or location from this image.\",\n",
      "  \"text_visible\": \"\"\n",
      "}\n",
      "  Analyzing segment 2...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [],\n",
      "  \"objects\": [],\n",
      "  \"date_estimate\": \"early 1900s\",\n",
      "  \"scene_description\": \"The image is extremely blurry and abstract, making it impossible to discern any specific details. It appears to be a close-up of a curved, possibly metallic, surface with reflections or light streaks.\",\n",
      "  \"text_visible\": \"none\"\n",
      "}\n",
      "  Analyzing segment 3...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [\n",
      "    {\n",
      "      \"name\": \"Turin\",\n",
      "      \"type\": \"city\"\n",
      "    }\n",
      "  ],\n",
      "  \"objects\": [\n",
      "    \"large metal rings\",\n",
      "    \"industrial machinery (partially visible)\",\n",
      "    \"floor\"\n",
      "  ],\n",
      "  \"date_estimate\": \"1900s\",\n",
      "  \"scene_description\": \"A close-up, angled view of large, concentric metal rings, likely part of industrial machinery or manufacturing process, in what appears to be a workshop or factory setting.\",\n",
      "  \"text_visible\": \"None\"\n",
      "}\n",
      " Album A_038 located_at Turin\n",
      "  Completed\n",
      "\n",
      "[20/20] \n",
      "============================================================\n",
      "Processing: Album A_041.JPG (album_page)\n",
      "  Segmented into 1 photos\n",
      "  Analyzing segment 1...\n",
      "    Gemini response: {\n",
      "  \"people\": [],\n",
      "  \"places\": [\n",
      "    {\n",
      "      \"name\": \"Turin\",\n",
      "      \"type\": \"city\"\n",
      "    }\n",
      "  ],\n",
      "  \"objects\": [\n",
      "    \"electrical control panel\",\n",
      "    \"large machinery with wheels/dials\",\n",
      "    \"pipes and conduits\",\n",
      "    \"chair\",\n",
      "    \"table/desk\",\n",
      "    \"windows\",\n",
      "    \"electrical wires\"\n",
      "  ],\n",
      "  \"date_estimate\": \"1900s-1920s\",\n",
      "  \"scene_description\": \"An interior view of an early 20th-century electrical power room or industrial control room with large machinery, a control panel, and a checkered floor. Sunlight streams through a window.\",\n",
      "  \"text_visible\": \"\"\n",
      "}\n",
      " Album A_041 located_at Turin\n",
      "  Completed\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CREATING DOCUMENT-TO-DOCUMENT RELATIONSHIPS\n",
      "============================================================\n",
      "  2025 - 10 - 17 alberi geneologici famiglia Martiny <-> 2025 - 10 - 20 Tag album giovani musicisti Martiny(1) (shares_person)\n",
      "  2025 - 10 - 17 alberi geneologici famiglia Martiny <-> 2025 - 10 - 17 articolo di giornalista conoscente di Roberto sulla famiglia Martiny (shares_person)\n",
      "  2025 - 10 - 20 Tag album giovani musicisti Martiny(1) <-> 2025 - 10 - 17 articolo di giornalista conoscente di Roberto sulla famiglia Martiny (shares_person)\n",
      "  Album A_000 <-> Album A_001 (same_album)\n",
      "  Album A_000 <-> Album A_006 (same_album)\n",
      "  Album A_000 <-> Album A_014 (same_album)\n",
      "  Album A_000 <-> Album A_017 (same_album)\n",
      "  Album A_000 <-> Album A_021 (same_album)\n",
      "  Album A_000 <-> Album A_023 (same_album)\n",
      "  Album A_000 <-> Album A_024 (same_album)\n",
      "  Album A_000 <-> Album A_038 (same_album)\n",
      "  Album A_000 <-> Album A_041 (same_album)\n",
      "  Album A_001 <-> Album A_006 (same_album)\n",
      "  Album A_001 <-> Album A_014 (same_album)\n",
      "  Album A_001 <-> Album A_017 (same_album)\n",
      "  Album A_001 <-> Album A_021 (same_album)\n",
      "  Album A_001 <-> Album A_023 (same_album)\n",
      "  Album A_001 <-> Album A_024 (same_album)\n",
      "  Album A_001 <-> Album A_038 (same_album)\n",
      "  Album A_001 <-> Album A_041 (same_album)\n",
      "  Album A_006 <-> Album A_014 (same_album)\n",
      "   Album A_006 <-> Album A_017 (shares_place)\n",
      "  Album A_006 <-> Album A_017 (same_album)\n",
      "  Album A_006 <-> Album A_021 (same_album)\n",
      "  Album A_006 <-> Album A_023 (same_album)\n",
      "  Album A_006 <-> Album A_024 (same_album)\n",
      "   Album A_006 <-> Album A_038 (shares_place)\n",
      "  Album A_006 <-> Album A_038 (same_album)\n",
      "   Album A_006 <-> Album A_041 (shares_place)\n",
      "  Album A_006 <-> Album A_041 (same_album)\n",
      "  Album A_014 <-> Album A_017 (same_album)\n",
      "  Album A_014 <-> Album A_021 (same_album)\n",
      "  Album A_014 <-> Album A_023 (same_album)\n",
      "  Album A_014 <-> Album A_024 (same_album)\n",
      "  Album A_014 <-> Album A_038 (same_album)\n",
      "  Album A_014 <-> Album A_041 (same_album)\n",
      "  Album A_017 <-> Album A_021 (same_album)\n",
      "  Album A_017 <-> Album A_023 (same_album)\n",
      "  Album A_017 <-> Album A_024 (same_album)\n",
      "   Album A_017 <-> Album A_038 (shares_place)\n",
      "  Album A_017 <-> Album A_038 (same_album)\n",
      "   Album A_017 <-> Album A_041 (shares_place)\n",
      "  Album A_017 <-> Album A_041 (same_album)\n",
      "  Album A_021 <-> Album A_023 (same_album)\n",
      "  Album A_021 <-> Album A_024 (same_album)\n",
      "  Album A_021 <-> Album A_038 (same_album)\n",
      "  Album A_021 <-> Album A_041 (same_album)\n",
      "  Album A_023 <-> Album A_024 (same_album)\n",
      "  Album A_023 <-> Album A_038 (same_album)\n",
      "  Album A_023 <-> Album A_041 (same_album)\n",
      "  Album A_024 <-> Album A_038 (same_album)\n",
      "  Album A_024 <-> Album A_041 (same_album)\n",
      "   Album A_038 <-> Album A_041 (shares_place)\n",
      "  Album A_038 <-> Album A_041 (same_album)\n",
      "\n",
      "============================================================\n",
      " Created 54 document-to-document relationships\n",
      "============================================================\n",
      "\n",
      "\n",
      "FINAL STATISTICS:\n",
      "  Entities: 47\n",
      "  Relationships: 87 (including 54 doc-to-doc)\n",
      "  Files Processed: 19\n"
     ]
    }
   ],
   "source": [
    "def create_document_to_document_relationships():\n",
    "    \"\"\"Create indirect relationships between documents that share entities\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING DOCUMENT-TO-DOCUMENT RELATIONSHIPS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get all document and photo entities\n",
    "    all_docs = [e for e in entities_db.values() if e.type in [\"Document\", \"Photo\"]]\n",
    "    created_count = 0\n",
    "    \n",
    "    # Compare each pair of documents\n",
    "    for i, doc1 in enumerate(all_docs):\n",
    "        for doc2 in all_docs[i+1:]:\n",
    "            \n",
    "            # 1. Check if documents share people\n",
    "            doc1_people = set()\n",
    "            doc2_people = set()\n",
    "            for rel in relationships_db:\n",
    "                # Document mentions person\n",
    "                if rel.source_id == doc1.id and rel.target_id.startswith(\"Person_\"):\n",
    "                    doc1_people.add(rel.target_id)\n",
    "                if rel.source_id == doc2.id and rel.target_id.startswith(\"Person_\"):\n",
    "                    doc2_people.add(rel.target_id)\n",
    "                # Person appears in document (reverse direction)\n",
    "                if rel.target_id == doc1.id and rel.source_id.startswith(\"Person_\"):\n",
    "                    doc1_people.add(rel.source_id)\n",
    "                if rel.target_id == doc2.id and rel.source_id.startswith(\"Person_\"):\n",
    "                    doc2_people.add(rel.source_id)\n",
    "            \n",
    "            shared_people = doc1_people & doc2_people\n",
    "            if shared_people:\n",
    "                person_names = [entities_db[pid].name for pid in shared_people]\n",
    "                relationships_db.append(Relationship(\n",
    "                    source_id=doc1.id, target_id=doc2.id, type=\"shares_person\",\n",
    "                    properties={\"person_ids\": list(shared_people), \"reason\": f\"Both mention {', '.join(person_names)}\"},\n",
    "                    confidence=0.92\n",
    "                ))\n",
    "                created_count += 1\n",
    "                print(f\"  {doc1.name} <-> {doc2.name} (shares_person)\")\n",
    "            \n",
    "            # 2. Check if documents share places\n",
    "            doc1_places = set()\n",
    "            doc2_places = set()\n",
    "            for rel in relationships_db:\n",
    "                if rel.source_id == doc1.id and rel.target_id.startswith(\"Place_\"):\n",
    "                    doc1_places.add(rel.target_id)\n",
    "                if rel.source_id == doc2.id and rel.target_id.startswith(\"Place_\"):\n",
    "                    doc2_places.add(rel.target_id)\n",
    "            \n",
    "            shared_places = doc1_places & doc2_places\n",
    "            if shared_places:\n",
    "                place_names = [entities_db[pid].name for pid in shared_places]\n",
    "                relationships_db.append(Relationship(\n",
    "                    source_id=doc1.id, target_id=doc2.id, type=\"shares_place\",\n",
    "                    properties={\"place_ids\": list(shared_places), \"reason\": f\"Both reference {', '.join(place_names)}\"},\n",
    "                    confidence=0.88\n",
    "                ))\n",
    "                created_count += 1\n",
    "                print(f\"   {doc1.name} <-> {doc2.name} (shares_place)\")\n",
    "            \n",
    "            # 3. Check if documents share companies\n",
    "            doc1_companies = set()\n",
    "            doc2_companies = set()\n",
    "            for rel in relationships_db:\n",
    "                if rel.source_id == doc1.id and rel.target_id.startswith(\"Company_\"):\n",
    "                    doc1_companies.add(rel.target_id)\n",
    "                if rel.source_id == doc2.id and rel.target_id.startswith(\"Company_\"):\n",
    "                    doc2_companies.add(rel.target_id)\n",
    "            \n",
    "            shared_companies = doc1_companies & doc2_companies\n",
    "            if shared_companies:\n",
    "                company_names = [entities_db[cid].name for cid in shared_companies]\n",
    "                relationships_db.append(Relationship(\n",
    "                    source_id=doc1.id, target_id=doc2.id, type=\"shares_company\",\n",
    "                    properties={\"company_ids\": list(shared_companies), \"reason\": f\"Both mention {', '.join(company_names)}\"},\n",
    "                    confidence=0.90\n",
    "                ))\n",
    "                created_count += 1\n",
    "                print(f\"  {doc1.name} <-> {doc2.name} (shares_company)\")\n",
    "            \n",
    "            # 4. Check if photos are from the same album\n",
    "            if doc1.type == \"Photo\" and doc2.type == \"Photo\":\n",
    "                # Extract album name from filename (e.g., \"Album A\" from \"Album A_001\")\n",
    "                album1 = doc1.name.rsplit('_', 1)[0] if '_' in doc1.name else None\n",
    "                album2 = doc2.name.rsplit('_', 1)[0] if '_' in doc2.name else None\n",
    "                if album1 and album2 and album1 == album2:\n",
    "                    relationships_db.append(Relationship(\n",
    "                        source_id=doc1.id, target_id=doc2.id, type=\"same_album\",\n",
    "                        properties={\"album_id\": album1, \"reason\": f\"Both from {album1}\"},\n",
    "                        confidence=1.0\n",
    "                    ))\n",
    "                    created_count += 1\n",
    "                    print(f\"  {doc1.name} <-> {doc2.name} (same_album)\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" Created {created_count} document-to-document relationships\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    return created_count\n",
    "\n",
    "# Main processing loop - process all selected files\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING PIPELINE EXECUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, file_path in enumerate(sample_files, 1):\n",
    "    category = archive_df[archive_df['path'] == file_path]['category'].values[0]\n",
    "    print(f\"\\n[{idx}/{len(sample_files)}]\", end=' ')\n",
    "    \n",
    "    try:\n",
    "        process_file(file_path, category)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {str(e)[:100]}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create document-to-document relationships\n",
    "doc_doc_count = create_document_to_document_relationships()\n",
    "\n",
    "print(f\"\\nFINAL STATISTICS:\")\n",
    "print(f\"  Entities: {len(entities_db)}\")\n",
    "print(f\"  Relationships: {len(relationships_db)} (including {doc_doc_count} doc-to-doc)\")\n",
    "print(f\"  Files Processed: {len(files_db)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24e682f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENTITY DISTRIBUTION\n",
      "----------------------------------------\n",
      "Person          :  21\n",
      "Photo           :  10\n",
      "Document        :   9\n",
      "Place           :   4\n",
      "Company         :   3\n",
      "\n",
      "RELATIONSHIP DISTRIBUTION\n",
      "----------------------------------------\n",
      "same_album      :  45\n",
      "mentions        :  25\n",
      "located_at      :   7\n",
      "shares_place    :   6\n",
      "shares_person   :   3\n",
      "appears_in      :   1\n",
      "\n",
      "SAMPLE ENTITIES\n",
      "----------------------------------------\n",
      "\n",
      "Persons:\n",
      "  - Giovanni Martiny\n",
      "  - Johann\n",
      "  - Alwine\n",
      "\n",
      "Places:\n",
      "  - Turin\n",
      "  - Walter Martiny Industria Gomma\n",
      "  - Società Anonima Walter Martiny Industria Gomma\n",
      "\n",
      "Companys:\n",
      "  - Bender e Martiny\n",
      "  - LA STAMPA\n",
      "  - PHILIPS\n"
     ]
    }
   ],
   "source": [
    "# Display entity distribution\n",
    "entity_types = defaultdict(int)\n",
    "for entity in entities_db.values():\n",
    "    entity_types[entity.type] += 1\n",
    "\n",
    "print(\"\\nENTITY DISTRIBUTION\")\n",
    "print(\"-\" * 40)\n",
    "for etype, count in sorted(entity_types.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{etype:15} : {count:3}\")\n",
    "\n",
    "# Display relationship distribution\n",
    "rel_types = defaultdict(int)\n",
    "for rel in relationships_db:\n",
    "    rel_types[rel.type] += 1\n",
    "\n",
    "print(\"\\nRELATIONSHIP DISTRIBUTION\")\n",
    "print(\"-\" * 40)\n",
    "for rtype, count in sorted(rel_types.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{rtype:15} : {count:3}\")\n",
    "\n",
    "# Show sample entities\n",
    "print(\"\\nSAMPLE ENTITIES\")\n",
    "print(\"-\" * 40)\n",
    "for entity_type in ['Person', 'Place', 'Company']:\n",
    "    samples = [e for e in entities_db.values() if e.type == entity_type][:3]\n",
    "    if samples:\n",
    "        print(f\"\\n{entity_type}s:\")\n",
    "        for e in samples:\n",
    "            print(f\"  - {e.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d84fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Knowledge Graph Built\n",
      "  Nodes: 47\n",
      "  Edges: 87\n",
      "  Connected components: 8\n"
     ]
    }
   ],
   "source": [
    "def networkx_graph() -> nx.MultiDiGraph:\n",
    "    \"\"\"Convert our entities and relationships into a NetworkX graph\"\"\"\n",
    "    G = nx.MultiDiGraph()  # graph that allows multiple edges\n",
    "    \n",
    "    # Add all entities as nodes\n",
    "    for entity_id, entity in entities_db.items():\n",
    "        G.add_node(entity_id, \n",
    "                   label=entity.name,\n",
    "                   entity_type=entity.type,\n",
    "                   **entity.properties)\n",
    "    \n",
    "    # Add all relationships as edges\n",
    "    for rel in relationships_db:\n",
    "        if rel.source_id in G.nodes and rel.target_id in G.nodes:\n",
    "            G.add_edge(rel.source_id, rel.target_id,\n",
    "                      relation_type=rel.type,\n",
    "                      **rel.properties)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Build the knowledge graph\n",
    "kg_graph = networkx_graph()\n",
    "\n",
    "print(f\"\\nKnowledge Graph Built\")\n",
    "print(f\"  Nodes: {kg_graph.number_of_nodes()}\")\n",
    "print(f\"  Edges: {kg_graph.number_of_edges()}\")\n",
    "print(f\"  Connected components: {nx.number_weakly_connected_components(kg_graph)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b28ca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced visualization saved: C:\\Users\\wel\\Desktop\\opvn\\outputs\\knowledge_graph.html\n",
      "\n",
      "Open C:\\Users\\wel\\Desktop\\opvn\\outputs\\knowledge_graph.html in your browser\n",
      "\n",
      "Graph Summary:\n",
      "  - People: 21\n",
      "  - Companies: 3\n",
      "  - Documents: 9\n",
      "  - Photos: 10\n"
     ]
    }
   ],
   "source": [
    "def create_interactive_visualization(graph: nx.MultiDiGraph, output_path: str) -> None:\n",
    "    \"\"\"Create an interactive HTML visualization of the knowledge graph\"\"\"\n",
    "    net = Network(height=\"900px\", width=\"100%\", directed=True, notebook=False, \n",
    "                  bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "    \n",
    "    # Configure physics for better layout\n",
    "    net.barnes_hut(\n",
    "        gravity=-80000,\n",
    "        central_gravity=0.3,\n",
    "        spring_length=200,\n",
    "        spring_strength=0.001,\n",
    "        damping=0.09,\n",
    "        overlap=0\n",
    "    )\n",
    "    \n",
    "    # Color scheme for different entity types\n",
    "    colors = {\n",
    "        'Person': '#FF6B6B',   \n",
    "        'Place': '#4ECDC4',  \n",
    "        'Company': '#45B7D1',   \n",
    "        'Document': '#96CEB4',   \n",
    "        'Photo': '#FFEAA7',       \n",
    "        'Event': '#DDA15E',       \n",
    "        'Product': '#BC6C25'     \n",
    "    }\n",
    "    \n",
    "    # Size scheme for different entity types\n",
    "    sizes = {\n",
    "        'Person': 30,\n",
    "        'Company': 25,\n",
    "        'Place': 20,\n",
    "        'Document': 15,\n",
    "        'Photo': 15,\n",
    "        'Event': 20,\n",
    "        'Product': 18\n",
    "    }\n",
    "    \n",
    "    # Add nodes to the visualization\n",
    "    for node_id, node_data in graph.nodes(data=True):\n",
    "        entity_type = node_data.get('entity_type', 'Document')\n",
    "        color = colors.get(entity_type, '#95a5a6')\n",
    "        size = sizes.get(entity_type, 15)\n",
    "        label = node_data.get('label', node_id)\n",
    "        \n",
    "        # Create tooltip with entity properties\n",
    "        properties_str = \"\"\n",
    "        for key, val in node_data.items():\n",
    "            if key not in ['label', 'entity_type'] and val:\n",
    "                properties_str += f\"<br><b>{key}:</b> {val}\"\n",
    "        \n",
    "        title = f\"<b>{entity_type}: {label}</b>{properties_str}\"\n",
    "        \n",
    "        net.add_node(\n",
    "            node_id, \n",
    "            label=label,\n",
    "            color=color,\n",
    "            title=title,\n",
    "            size=size,\n",
    "            font={'size': 14, 'face': 'arial', 'color': '#000000'},\n",
    "            shape='dot',\n",
    "            borderWidth=2,\n",
    "            borderWidthSelected=4\n",
    "        )\n",
    "    \n",
    "    # Add edges to the visualization\n",
    "    for source, target, edge_data in graph.edges(data=True):\n",
    "        relation_type = edge_data.get('relation_type', 'related_to')\n",
    "        \n",
    "        # Create tooltip with edge properties\n",
    "        props_str = \"\"\n",
    "        for key, val in edge_data.items():\n",
    "            if key != 'relation_type' and val:\n",
    "                props_str += f\"<br>{key}: {val}\"\n",
    "        \n",
    "        edge_title = f\"<b>{relation_type}</b>{props_str}\"\n",
    "        \n",
    "        net.add_edge(\n",
    "            source, \n",
    "            target,\n",
    "            title=edge_title,\n",
    "            label=relation_type,\n",
    "            arrows='to',\n",
    "            font={'size': 10, 'align': 'middle'},\n",
    "            color={'color': '#848484', 'highlight': '#FF0000'},\n",
    "            width=2,\n",
    "            smooth={'type': 'dynamic'}\n",
    "        )\n",
    "    \n",
    "    # Calculate statistics for the summary panel\n",
    "    entity_counts = defaultdict(int)\n",
    "    for _, node_data in graph.nodes(data=True):\n",
    "        entity_counts[node_data.get('entity_type', 'Unknown')] += 1\n",
    "    \n",
    "    relation_counts = defaultdict(int)\n",
    "    for _, _, edge_data in graph.edges(data=True):\n",
    "        relation_counts[edge_data.get('relation_type', 'unknown')] += 1\n",
    "    \n",
    "    # Create HTML summary panel\n",
    "    summary_html = f\"\"\"\n",
    "    <div style=\"padding: 20px; background-color: #f8f9fa; border-bottom: 2px solid #dee2e6;\">\n",
    "        <h1 style=\"text-align: center; color: #333;\">Martiny Family Archive - Knowledge Graph</h1>\n",
    "        \n",
    "        <div style=\"display: flex; justify-content: space-around; margin-top: 20px;\">\n",
    "            <div style=\"text-align: center;\">\n",
    "                <h3 style=\"color: #666;\">Graph Statistics</h3>\n",
    "                <p><strong>Total Nodes:</strong> {graph.number_of_nodes()}</p>\n",
    "                <p><strong>Total Relationships:</strong> {graph.number_of_edges()}</p>\n",
    "                <p><strong>Connected Components:</strong> {nx.number_weakly_connected_components(graph)}</p>\n",
    "            </div>\n",
    "            \n",
    "            <div style=\"text-align: center;\">\n",
    "                <h3 style=\"color: #666;\">Entity Types</h3>\n",
    "                {''.join([f'<p><span style=\"color: {colors.get(etype, \"#999\")};\">●</span> <strong>{etype}:</strong> {count}</p>' \n",
    "                          for etype, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)])}\n",
    "            </div>\n",
    "            \n",
    "            <div style=\"text-align: center;\">\n",
    "                <h3 style=\"color: #666;\">Relationship Types</h3>\n",
    "                {''.join([f'<p><strong>{rtype}:</strong> {count}</p>' \n",
    "                          for rtype, count in sorted(relation_counts.items(), key=lambda x: x[1], reverse=True)])}\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"text-align: center; margin-top: 15px; padding: 10px; background-color: #e9ecef; border-radius: 5px;\">\n",
    "            <p style=\"margin: 5px;\"><strong>Controls:</strong> \n",
    "            Drag nodes to reposition | Scroll to zoom | Click and drag background to pan | Hover over nodes/edges for details</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save the graph\n",
    "    net.save_graph(output_path)\n",
    "    \n",
    "    # Inject our custom summary HTML into the generated file\n",
    "    with open(output_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    html_content = html_content.replace(\n",
    "        '<div class=\"card\" style=\"width: 100%\">',\n",
    "        f'{summary_html}<div class=\"card\" style=\"width: 100%\">'\n",
    "    )\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"Enhanced visualization saved: {output_path}\")\n",
    "\n",
    "# Create the visualization\n",
    "viz_path = str(Path(OUTPUT_DIR) / \"knowledge_graph.html\")\n",
    "create_interactive_visualization(kg_graph, viz_path)\n",
    "print(f\"\\nOpen {viz_path} in your browser\")\n",
    "\n",
    "# Print entity type summary\n",
    "print(f\"\\nGraph Summary:\")\n",
    "print(f\"  - People: {sum(1 for _, d in kg_graph.nodes(data=True) if d.get('entity_type') == 'Person')}\")\n",
    "print(f\"  - Companies: {sum(1 for _, d in kg_graph.nodes(data=True) if d.get('entity_type') == 'Company')}\")\n",
    "print(f\"  - Documents: {sum(1 for _, d in kg_graph.nodes(data=True) if d.get('entity_type') == 'Document')}\")\n",
    "print(f\"  - Photos: {sum(1 for _, d in kg_graph.nodes(data=True) if d.get('entity_type') == 'Photo')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14234bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 47 entities to entities.json\n",
      "Exported 87 relationships to relationships.json\n",
      "Exported 19 file records to file_metadata.json\n",
      "Exported statistics to statistics.json\n"
     ]
    }
   ],
   "source": [
    "# Export entities\n",
    "entities_export = [asdict(e) for e in entities_db.values()]\n",
    "with open(Path(OUTPUT_DIR) / \"entities.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(entities_export, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Exported {len(entities_export)} entities to entities.json\")\n",
    "\n",
    "# Export relationships\n",
    "relationships_export = [asdict(r) for r in relationships_db]\n",
    "with open(Path(OUTPUT_DIR) / \"relationships.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(relationships_export, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Exported {len(relationships_export)} relationships to relationships.json\")\n",
    "\n",
    "# Export file metadata\n",
    "files_export = [asdict(f) for f in files_db.values()]\n",
    "with open(Path(OUTPUT_DIR) / \"file_metadata.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(files_export, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Exported {len(files_export)} file records to file_metadata.json\")\n",
    "\n",
    "# Export comprehensive statistics\n",
    "stats = {\n",
    "    \"total_entities\": len(entities_db),\n",
    "    \"total_relationships\": len(relationships_db),\n",
    "    \"total_files_processed\": len(files_db),\n",
    "    \"entity_types\": dict(entity_types),\n",
    "    \"relationship_types\": dict(rel_types),\n",
    "    \"graph_stats\": {\n",
    "        \"nodes\": kg_graph.number_of_nodes(),\n",
    "        \"edges\": kg_graph.number_of_edges(),\n",
    "        \"density\": nx.density(kg_graph),\n",
    "        \"connected_components\": nx.number_weakly_connected_components(kg_graph)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(Path(OUTPUT_DIR) / \"statistics.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "print(f\"Exported statistics to statistics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d620b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION REPORT\n",
      "============================================================\n",
      "Status: PASS\n",
      "\n",
      "Graph Structure:\n",
      "  orphaned_nodes: 4\n",
      "\n",
      "Data Quality:\n",
      "  entities_with_properties: 47\n",
      "  completeness_ratio: 1.0\n",
      "  potential_duplicates: 0\n",
      "\n",
      "Extraction Quality:\n",
      "  files_processed: 19\n",
      "  files_with_entities: 0\n",
      "\n",
      "Issues Found:\n",
      "  - 4 orphaned nodes found\n",
      "\n",
      "Validation report saved\n",
      "\n",
      "All outputs saved to: C:\\Users\\wel\\Desktop\\opvn\\outputs\n",
      "\n",
      "Pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "def validate_knowledge_graph() -> Dict:\n",
    "    \"\"\"Run quality checks on the generated knowledge graph\"\"\"\n",
    "    validation = {\n",
    "        \"extraction_quality\": {},\n",
    "        \"graph_structure\": {},\n",
    "        \"data_quality\": {},\n",
    "        \"issues\": []\n",
    "    }\n",
    "    \n",
    "    # Check for orphaned nodes \n",
    "    orphaned = [n for n in kg_graph.nodes() if kg_graph.degree(n) == 0]\n",
    "    validation[\"graph_structure\"][\"orphaned_nodes\"] = len(orphaned)\n",
    "    if len(orphaned) > 0:\n",
    "        validation[\"issues\"].append(f\"{len(orphaned)} orphaned nodes found\")\n",
    "    \n",
    "    # Check entity completeness (entities should have properties)\n",
    "    entities_with_props = sum(1 for e in entities_db.values() if e.properties)\n",
    "    validation[\"data_quality\"][\"entities_with_properties\"] = entities_with_props\n",
    "    validation[\"data_quality\"][\"completeness_ratio\"] = entities_with_props / len(entities_db)\n",
    "    \n",
    "    # Check for potential duplicates\n",
    "    names = [e.name for e in entities_db.values()]\n",
    "    duplicates = len(names) - len(set(names))\n",
    "    validation[\"data_quality\"][\"potential_duplicates\"] = duplicates\n",
    "    \n",
    "    # Check file processing success rate\n",
    "    files_with_entities = sum(1 for f in files_db.values() if f.extracted_entities)\n",
    "    validation[\"extraction_quality\"][\"files_processed\"] = len(files_db)\n",
    "    validation[\"extraction_quality\"][\"files_with_entities\"] = files_with_entities\n",
    "    \n",
    "    # Overall status\n",
    "    validation[\"status\"] = \"PASS\" if len(validation[\"issues\"]) < 3 else \"WARNING\"\n",
    "    \n",
    "    return validation\n",
    "\n",
    "# Run validation\n",
    "validation_report = validate_knowledge_graph()\n",
    "\n",
    "print(\"\\nVALIDATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Status: {validation_report['status']}\")\n",
    "\n",
    "print(f\"\\nGraph Structure:\")\n",
    "for k, v in validation_report['graph_structure'].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nData Quality:\")\n",
    "for k, v in validation_report['data_quality'].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nExtraction Quality:\")\n",
    "for k, v in validation_report['extraction_quality'].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "if validation_report['issues']:\n",
    "    print(f\"\\nIssues Found:\")\n",
    "    for issue in validation_report['issues']:\n",
    "        print(f\"  - {issue}\")\n",
    "\n",
    "# Save validation report\n",
    "with open(Path(OUTPUT_DIR) / \"validation_report.json\", 'w') as f:\n",
    "    json.dump(validation_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nValidation report saved\")\n",
    "print(f\"\\nAll outputs saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nPipeline complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68018a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf130416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ddda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
